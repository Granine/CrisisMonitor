{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEaBUa04cYIK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilBERT baseline using HuggingFace"
      ],
      "metadata": {
        "id": "T50W7WTxc-OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers accelerate datasets scikit-learn"
      ],
      "metadata": {
        "id": "ApK-S0niehhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, platform\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available(), \"| Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "print(\"Python:\", platform.python_version())"
      ],
      "metadata": {
        "id": "zhYIvhiLekrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, datasets, accelerate\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"accelerate:\", accelerate.__version__)"
      ],
      "metadata": {
        "id": "uDjCCbjwe6zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# DistilBERT baseline for \"NLP with Disaster Tweets\"\n",
        "# =========================\n",
        "import os, re, random, numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from datasets import Dataset\n",
        "from transformers import EarlyStoppingCallback\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -------------------------\n",
        "# Data paths\n",
        "# -------------------------\n",
        "TRAIN_PATH = '/content/drive/MyDrive/University of Toronto/CSC2701 Communication for Computer Scientists/Kaggle Dataset/train.csv'\n",
        "TEST_PATH  = '/content/drive/MyDrive/University of Toronto/CSC2701 Communication for Computer Scientists/Kaggle Dataset/test.csv'\n",
        "SUB_PATH   = \"submission.csv\"\n",
        "\n",
        "# -------------------------\n",
        "# Light tweet normalization, can add any cleaning or preprocessin here\n",
        "# -------------------------\n",
        "URL_RE  = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "USER_RE = re.compile(r\"@\\w+\")\n",
        "def normalize_tweet(t: str) -> str:\n",
        "    t = URL_RE.sub(\" <url> \", t)\n",
        "    t = USER_RE.sub(\" <user> \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "# -------------------------\n",
        "# Load data\n",
        "# -------------------------\n",
        "df = pd.read_csv(TRAIN_PATH)\n",
        "df[\"text\"] = df[\"text\"].astype(str).map(normalize_tweet)\n",
        "\n",
        "df_train, df_val = train_test_split(\n",
        "    df, test_size=0.1, random_state=SEED, stratify=df[\"target\"]\n",
        ")\n",
        "\n",
        "df_test = pd.read_csv(TEST_PATH)\n",
        "df_test[\"text\"] = df_test[\"text\"].astype(str).map(normalize_tweet)\n",
        "\n",
        "# -------------------------\n",
        "# Hugging Face Datasets\n",
        "# -------------------------\n",
        "train_ds = Dataset.from_pandas(df_train[[\"text\", \"target\"]].reset_index(drop=True))\n",
        "val_ds   = Dataset.from_pandas(df_val[[\"text\", \"target\"]].reset_index(drop=True))\n",
        "test_ds  = Dataset.from_pandas(df_test[[\"id\", \"text\"]].reset_index(drop=True))\n",
        "\n",
        "# -------------------------\n",
        "# Model & tokenizer\n",
        "# -------------------------\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "NUM_LABELS = 2\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "config    = AutoConfig.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
        "model     = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "MAX_LEN   = 128\n",
        "\n",
        "def tok_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=MAX_LEN,\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
        "val_ds   = val_ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
        "test_ds  = test_ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Rename target -> labels for Trainer\n",
        "train_ds = train_ds.rename_column(\"target\", \"labels\")\n",
        "val_ds   = val_ds.rename_column(\"target\", \"labels\")\n",
        "\n",
        "# Data collator pads dynamically\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# -------------------------\n",
        "# Metrics\n",
        "# -------------------------\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = (logits.argmax(axis=-1)).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds)\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Training arguments\n",
        "# -------------------------\n",
        "BATCH = 32\n",
        "EPOCHS = 10\n",
        "LR = 1e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./distilbert-disaster\",\n",
        "    eval_strategy=\"epoch\",   # <- REQUIRED if load_best_model_at_end=True\n",
        "    save_strategy=\"epoch\",         # usually match eval strategy\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",    # must match key from compute_metrics\n",
        "    greater_is_better=True,\n",
        "\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    per_device_eval_batch_size=BATCH,\n",
        "    learning_rate=LR,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    max_grad_norm=1.0,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,            # optional: keep only best + last\n",
        "    report_to=[],                  # disable W&B/etc\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Trainer\n",
        "# -------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(\"Validation metrics:\", metrics)\n",
        "\n",
        "# -------------------------\n",
        "# Tune Decision Threshold\n",
        "# -------------------------\n",
        "\n",
        "val_pred = trainer.predict(val_ds)  # returns PredictionOutput with .predictions\n",
        "logits = val_pred.predictions  # shape [N, 2] for softmax models\n",
        "y_true = val_ds[\"labels\"] if \"labels\" in val_ds.features else df_val[\"target\"].to_numpy()\n",
        "\n",
        "probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
        "\n",
        "best = {\"f1\": -1, \"thr\": 0.5, \"prec\": 0.0, \"rec\": 0.0}\n",
        "for thr in np.linspace(0.2, 0.8, 61):  # dense sweep\n",
        "    y_pred = (probs >= thr).astype(int)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    if f1 > best[\"f1\"] or (np.isclose(f1, best[\"f1\"]) and rec > best[\"rec\"]):\n",
        "        best = {\"f1\": float(f1), \"thr\": float(thr), \"prec\": float(prec), \"rec\": float(rec)}\n",
        "\n",
        "print(f\"Best threshold = {best['thr']:.3f} | F1={best['f1']:.4f} (P={best['prec']:.4f}, R={best['rec']:.4f})\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Inference on test & submission\n",
        "# -------------------------\n",
        "# preds = trainer.predict(test_ds).predictions\n",
        "# test_labels = preds.argmax(axis=-1)\n",
        "\n",
        "test_logits = trainer.predict(test_ds).predictions\n",
        "test_probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()[:, 1]\n",
        "test_labels = (test_probs >= best[\"thr\"]).astype(int)\n",
        "\n",
        "# sub = pd.DataFrame({\n",
        "#     \"id\": df_test[\"id\"],\n",
        "#     \"target\": test_labels.astype(int)\n",
        "# })\n",
        "# sub.to_csv(SUB_PATH, index=False)\n",
        "# print(f\"Saved {SUB_PATH} with shape {sub.shape}\")\n",
        "\n",
        "sub = pd.DataFrame({\"id\": df_test[\"id\"], \"target\": test_labels})\n",
        "sub.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Saved submission.csv with tuned threshold\", best[\"thr\"])\n"
      ],
      "metadata": {
        "id": "XJDTVqvTczvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Inference prompt for trained DistilBERT disaster classifier\n",
        "# =========================\n",
        "import os, re, glob, json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ---- set this to your training output_dir ----\n",
        "OUTPUT_DIR = \"./distilbert-disaster\"\n",
        "\n",
        "# ---- find the best checkpoint (from trainer_state.json), fall back to latest, then to OUTPUT_DIR ----\n",
        "def resolve_model_dir(output_dir: str) -> str:\n",
        "    state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
        "    if os.path.exists(state_path):\n",
        "        try:\n",
        "            with open(state_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                st = json.load(f)\n",
        "            best_ckpt = st.get(\"best_model_checkpoint\")\n",
        "            if best_ckpt and os.path.isdir(best_ckpt):\n",
        "                return best_ckpt\n",
        "        except Exception:\n",
        "            pass\n",
        "    ckpts = [p for p in glob.glob(os.path.join(output_dir, \"checkpoint-*\")) if os.path.isdir(p)]\n",
        "    if ckpts:\n",
        "        ckpts.sort(key=lambda p: int(p.split(\"-\")[-1]))\n",
        "        return ckpts[-1]\n",
        "    return output_dir  # last resort (e.g., if you saved directly to OUTPUT_DIR)\n",
        "\n",
        "MODEL_DIR = resolve_model_dir(OUTPUT_DIR)\n",
        "print(f\"Loading model from: {MODEL_DIR}\")\n",
        "\n",
        "# ---- load tokenizer/model ----\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# ---- same light normalization as training ----\n",
        "URL_RE  = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "USER_RE = re.compile(r\"@\\w+\")\n",
        "def normalize_tweet(t: str) -> str:\n",
        "    t = URL_RE.sub(\" <url> \", str(t))\n",
        "    t = USER_RE.sub(\" <user> \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "# ---- label mapping (0/1 -> human-readable) ----\n",
        "IDX2LABEL = {0: \"not disaster\", 1: \"disaster\"}\n",
        "\n",
        "# ---- single-text prediction ----\n",
        "@torch.no_grad()\n",
        "def predict_one(text: str, max_len: int = 128):\n",
        "    text = normalize_tweet(text)\n",
        "    enc = tokenizer(text, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "    logits = model(**enc).logits\n",
        "    probs = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
        "    pred = int(np.argmax(probs))\n",
        "    return pred, probs\n",
        "\n",
        "# ---- interactive loop ----\n",
        "print(\"\\nType a tweet and press Enter to classify. Type 'quit' (or empty line) to exit.\\n\")\n",
        "while True:\n",
        "    try:\n",
        "        user_text = input(\"Tweet> \").strip()\n",
        "    except EOFError:\n",
        "        break\n",
        "    if user_text.lower() in {\"\", \"quit\", \"exit\"}:\n",
        "        print(\"Bye!\")\n",
        "        break\n",
        "    pred, probs = predict_one(user_text, max_len=128)\n",
        "    print(f\"Prediction: {IDX2LABEL[pred]}  |  P(not disaster)={probs[0]:.3f}, P(disaster)={probs[1]:.3f}\\n\")\n"
      ],
      "metadata": {
        "id": "Yc95vyBcdnui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYARbD4Of9Is"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}